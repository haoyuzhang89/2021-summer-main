# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 5 parts for a total of 29 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Project Small Model (0 points)
# - Transformers (8 points)
# - BERT and Transfer Learning (5 points)
# - Practical Machine Learning (10 points)
# - Information Extraction (6 points)



###################################################################
###################################################################
## Project Small Model (0 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Project Progress (unrelated to a notebook) (0 points)  | 
# ------------------------------------------------------------------

# Question 1 (/0): What are the dimensions of your bottom most hidden layer?
project_small_model_a_1: [300, 4]

# Question 2 (/0): How many hidden layers are in your model?
project_small_model_a_2: In develope

# Question 3 (/0): What is your starting loss?
project_small_model_a_3: Not available

# Question 4 (/0): What is your ending loss?
project_small_model_a_4: Not available

# Question 5 (/0): How many epochs did you train?
project_small_model_a_5: 30(Plan)



###################################################################
###################################################################
## Transformers (8 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Transformers (8 points)  | 
# ------------------------------------------------------------------

# Question 1 (/2): How are Q, K, and V constructed in the Attention is All You Need Paper (Vaswani et. al.)?
# (This question is multiple choice.  Delete all but the correct answer).
transformers_a_1: 
 - Three learned linear transforms of the representation of each word

# Question 2 (/2): Q, K, V are three vectors used in the Attention is All You Need paper.  In the simple attention model, we only had a query and hidden state values.  Which of these played the role of the key in that simpler model?
# (This question is multiple choice.  Delete all but the correct answer).
transformers_a_2: 
 - hidden state values

# Question 3 (/2): A key difference between the encoder and decoder structures of the transformer in Vaswani et. al. is ...?
# (This question is multiple choice.  Delete all but the correct answer).
transformers_a_3: 
 - Masked multi-head attention

# Question 4 (/2): In the Vaswani et. al. transformer, how is the encoder output fed into the decoder...?
# (This question is multiple choice.  Delete all but the correct answer).
transformers_a_4: 
 - Through a second attention function in the decoder



###################################################################
###################################################################
## BERT and Transfer Learning (5 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Transfer Learning (1 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): Transfer learning is...
# (This question is multiple choice.  Delete all but the correct answer).
bert_and_transfer_learning_a_1: 
 - the use of a generalized model built on some task(s) being used to solve another task


# ------------------------------------------------------------------
# | Section (b): BERT (4 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): BERT is trained with a next sentence prediction task and...
# (This question is multiple choice.  Delete all but the correct answer).
bert_and_transfer_learning_b_1: 
 - a masked language model task

# Question 2 (/1): BERT solves the large vocabulary problem with...
# (This question is multiple choice.  Delete all but the correct answer).
bert_and_transfer_learning_b_2: 
 - wordpiece tokenization

# Question 3 (/1): BERT's decoder architecture allows it to generate seemingly fluent text. True or False?
# (This question is multiple choice.  Delete all but the correct answer).
bert_and_transfer_learning_b_3: 
 - False

# Question 4 (/1): The RoBERTa model is like BERT but ...
# (This question is multiple choice.  Delete all but the correct answer).
bert_and_transfer_learning_b_4: 
 - trained with better masking and more epochs



###################################################################
###################################################################
## Practical Machine Learning (10 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Activation functions (6 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is the derivative of sigmoid(z) as z approaches infinity?
practical_machine_learning_a_1: 0

# Question 2 (/1): What is the derivative of sigmoid(z) as z approaches -infinity?
practical_machine_learning_a_2: 0

# Question 3 (/2): What can this cause?
# (This question is multiple choice.  Delete all but the correct answer).
practical_machine_learning_a_3: 
 - Layers closest to the features in the network may not update much as the product of partial derivatives is close to zero

# Question 4 (/2): What can we do about this?
# (This question is multiple choice.  Delete all but the correct answer).
practical_machine_learning_a_4: 
 - All of the above


# ------------------------------------------------------------------
# | Section (c): Learning curves (4 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): Training loss goes to zero.  Eval loss is still very bad.  What is most likely wrong?
# (This question is multiple choice.  Delete all but the correct answer).
practical_machine_learning_c_1: 
 - Overfitting

# Question 2 (/1): Training and eval loss are high and barely move.  What is most likely wrong?
# (This question is multiple choice.  Delete all but the correct answer).
practical_machine_learning_c_2: 
 - Model has insufficient capacity

# Question 3 (/1): Training and eval loss go up, and keep going up.  What is most likely wrong?
# (This question is multiple choice.  Delete all but the correct answer).
practical_machine_learning_c_3: 
 - Learning rate too high

# Question 4 (/1): What is an efficient use of your time when first starting to train a model?
# (This question is multiple choice.  Delete all but the correct answer).
practical_machine_learning_c_4: 
 - Play with hyperparameters to find a starting point where it can memorize 1000 examples in a few minutes



###################################################################
###################################################################
## Information Extraction (6 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Overview (6 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What are the roles of BIO tags?
# (This question is multiple choice.  Delete all but the correct answer).
information_extraction_a_1: 
 - To indicate beginning and internal words involved in an entity

# Question 2 (/1): Why is coreference hard?
# (This question is multiple choice.  Delete all but the correct answer).
information_extraction_a_2: 
 - All of the above

# Question 3 (/1): Hobbs is a...
# (This question is multiple choice.  Delete all but the correct answer).
information_extraction_a_3: 
 - A heuristic to resolve coreferences often used as a feature in ML coref systems

# Question 4 (/1): A common technique in information extraction is to identify tuples of the form (entity1, relation, entity2).  How many tuples do you need to represent [James became the new janitor at UC Berkeley]?
information_extraction_a_4: 3

# Question 5 (/1): Named entity recognition and named entity resolution are two names for the same thing. True or False?
# (This question is multiple choice.  Delete all but the correct answer).
information_extraction_a_5: 
 - False

# Question 6 (/1): Open relation extraction (like the TextRunner system) is...?  (There is more than one right answer so leave all the correct ones.)
# (This question is multiple choice.  Delete all but the correct answer).
information_extraction_a_6: 
 - Relatively easy to implement
 - Does not require expensive training data
 - Only works on cases with lots of repitition
